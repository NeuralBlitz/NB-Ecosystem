## Models

- [Uncensored (free)](https://openrouter.ai/cognitivecomputations/dolphin-mistral-24b-venice-edition:free)
	1.36B tokens
	[
	Venice Uncensored Dolphin Mistral 24B Venice Edition is a fine-tuned variant of Mistral-Small-24B-Instruct-2501, developed by dphn.ai in collaboration with Venice.ai. This model is designed as an “uncensored” instruct-tuned LLM, preserving user control over alignment, system prompts, and behavior. Intended for advanced and unrestricted use cases, Venice Uncensored emphasizes steerability and transparent behavior, removing default safety and alignment layers typically found in mainstream assistant models.
	](https://openrouter.ai/cognitivecomputations/dolphin-mistral-24b-venice-edition:free)
	by [venice](https://openrouter.ai/venice) 33K context $0/M input $0/M output
- [Gemini 2.0 Flash Experimental (free)](https://openrouter.ai/google/gemini-2.0-flash-exp:free)
	1.29B tokens
	[
	Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to Gemini Flash 1.5, while maintaining quality on par with larger models like Gemini Pro 1.5. It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. These advancements come together to deliver more seamless and robust agentic experiences.
	](https://openrouter.ai/google/gemini-2.0-flash-exp:free)
	by [google](https://openrouter.ai/google) 1.05M context $0/M input $0/M output
- [Hermes 3 405B Instruct (free)](https://openrouter.ai/nousresearch/hermes-3-llama-3.1-405b:free)
	955M tokens
	[
	Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board. Hermes 3 405B is a frontier-level, full-parameter finetune of the Llama-3.1 405B foundation model, focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user. The Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills. Hermes 3 is competitive, if not superior, to Llama-3.1 Instruct models at general capabilities, with varying strengths and weaknesses attributable between the two.
	](https://openrouter.ai/nousresearch/hermes-3-llama-3.1-405b:free)
	by [nousresearch](https://openrouter.ai/nousresearch) 131K context $0/M input $0/M output
- [gpt-oss-20b (free)](https://openrouter.ai/openai/gpt-oss-20b:free)
	580M tokens
	[
	gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimized for lower-latency inference and deployability on consumer or single-GPU hardware. The model is trained in OpenAI’s Harmony response format and supports reasoning level configuration, fine-tuning, and agentic capabilities including function calling, tool use, and structured outputs.
	](https://openrouter.ai/openai/gpt-oss-20b:free)
	by [openai](https://openrouter.ai/openai) 131K context $0/M input $0/M output
- [Mistral 7B Instruct (free)](https://openrouter.ai/mistralai/mistral-7b-instruct:free)
	562M tokens
	[
	A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length. *Mistral 7B Instruct has multiple version variants, and this is intended to be the latest version.*
	](https://openrouter.ai/mistralai/mistral-7b-instruct:free)
	by [mistralai](https://openrouter.ai/mistralai) 33K context $0/M input $0/M output
- [Mistral Small 3.1 24B (free)](https://openrouter.ai/mistralai/mistral-small-3.1-24b-instruct:free)
	514M tokens
	[
	Mistral Small 3.1 24B Instruct is an upgraded variant of Mistral Small 3 (2501), featuring 24 billion parameters with advanced multimodal capabilities. It provides state-of-the-art performance in text-based reasoning and vision tasks, including image analysis, programming, mathematical reasoning, and multilingual support across dozens of languages. Equipped with an extensive 128k token context window and optimized for efficient local inference, it supports use cases such as conversational agents, function calling, long-document comprehension, and privacy-sensitive deployments. The updated version is Mistral Small 3.2
	](https://openrouter.ai/mistralai/mistral-small-3.1-24b-instruct:free)
	by [mistralai](https://openrouter.ai/mistralai) 128K context $0/M input $0/M output
- [Llama 3.1 405B Instruct (free)](https://openrouter.ai/meta-llama/llama-3.1-405b-instruct:free)
	408M tokens
	[
	The highly anticipated 400B class of Llama3 is here! Clocking in at 128k context with impressive eval scores, the Meta AI team continues to push the frontier of open-source LLMs. Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 405B instruct-tuned version is optimized for high quality dialogue usecases. It has demonstrated strong performance compared to leading closed-source models including GPT-4o and Claude 3.5 Sonnet in evaluations. To read more about the model release, click here. Usage of this model is subject to Meta's Acceptable Use Policy.
	](https://openrouter.ai/meta-llama/llama-3.1-405b-instruct:free)
	by [meta-llama](https://openrouter.ai/meta-llama) 131K context $0/M input $0/M output
- [Nemotron Nano 9B V2 (free)](https://openrouter.ai/nvidia/nemotron-nano-9b-v2:free)
	343M tokens
	[
	NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. The model's reasoning capabilities can be controlled via a system prompt. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so.
	](https://openrouter.ai/nvidia/nemotron-nano-9b-v2:free)
	by [nvidia](https://openrouter.ai/nvidia) 128K context $0/M input $0/M output
- [Trinity Mini (free)](https://openrouter.ai/arcee-ai/trinity-mini:free)
	300M tokens
	[
	Trinity Mini is a 26B-parameter (3B active) sparse mixture-of-experts language model featuring 128 experts with 8 active per token. Engineered for efficient reasoning over long contexts (131k) with robust function calling and multi-step agent workflows.
	](https://openrouter.ai/arcee-ai/trinity-mini:free)
	by [arcee-ai](https://openrouter.ai/arcee-ai) 131K context $0/M input $0/M output
- [Llama 3.2 3B Instruct (free)](https://openrouter.ai/meta-llama/llama-3.2-3b-instruct:free)
	154M tokens
	[
	Llama 3.2 3B is a 3-billion-parameter multilingual large language model, optimized for advanced natural language processing tasks like dialogue generation, reasoning, and summarization. Designed with the latest transformer architecture, it supports eight languages, including English, Spanish, and Hindi, and is adaptable for additional languages. Trained on 9 trillion tokens, the Llama 3.2 3B model excels in instruction-following, complex reasoning, and tool use. Its balanced performance makes it ideal for applications needing accuracy and efficiency in text generation across multilingual settings. Click here for the original model card. Usage of this model is subject to Meta's Acceptable Use Policy.
	](https://openrouter.ai/meta-llama/llama-3.2-3b-instruct:free)
	by [meta-llama](https://openrouter.ai/meta-llama) 131K context $0/M input $0/M output
- [Qwen3 4B (free)](https://openrouter.ai/qwen/qwen3-4b:free)
	147M tokens
	[
	Qwen3-4B is a 4 billion parameter dense language model from the Qwen3 series, designed to support both general-purpose and reasoning-intensive tasks. It introduces a dual-mode architecture—thinking and non-thinking—allowing dynamic switching between high-precision logical reasoning and efficient dialogue generation. This makes it well-suited for multi-turn chat, instruction following, and complex agent workflows.
	](https://openrouter.ai/qwen/qwen3-4b:free)
	by [qwen](https://openrouter.ai/qwen) 41K context $0/M input $0/M output
- [Qwen2.5-VL 7B Instruct (free)](https://openrouter.ai/qwen/qwen-2.5-vl-7b-instruct:free)
	121M tokens
	[
	Qwen2.5 VL 7B is a multimodal LLM from the Qwen Team with the following key enhancements: - SoTA understanding of images of various resolution & ratio: Qwen2.5-VL achieves state-of-the-art performance on visual understanding benchmarks, including MathVista, DocVQA, RealWorldQA, MTVQA, etc. - Understanding videos of 20min+: Qwen2.5-VL can understand videos over 20 minutes for high-quality video-based question answering, dialog, content creation, etc. - Agent that can operate your mobiles, robots, etc.: with the abilities of complex reasoning and decision making, Qwen2.5-VL can be integrated with devices like mobile phones, robots, etc., for automatic operation based on visual environment and text instructions. - Multilingual Support: to serve global users, besides English and Chinese, Qwen2.5-VL now supports the understanding of texts in different languages inside images, including most European languages, Japanese, Korean, Arabic, Vietnamese, etc. For more details, see this blog post and GitHub repo. Usage of this model is subject to Tongyi Qianwen LICENSE AGREEMENT.
	](https://openrouter.ai/qwen/qwen-2.5-vl-7b-instruct:free)
	by [qwen](https://openrouter.ai/qwen) 33K context $0/M input $0/M output
- [Gemma 3n 2B (free)](https://openrouter.ai/google/gemma-3n-e2b-it:free)
	65M tokens
	[
	Gemma 3n E2B IT is a multimodal, instruction-tuned model developed by Google DeepMind, designed to operate efficiently at an effective parameter size of 2B while leveraging a 6B architecture. Based on the MatFormer architecture, it supports nested submodels and modular composition via the Mix-and-Match framework. Gemma 3n models are optimized for low-resource deployment, offering 32K context length and strong multilingual and reasoning performance across common benchmarks. This variant is trained on a diverse corpus including code, math, web, and multimodal data.
	](https://openrouter.ai/google/gemma-3n-e2b-it:free)
	by [google](https://openrouter.ai/google) 8K context $0/M input $0/M output
- [Gemma 3 4B (free)](https://openrouter.ai/google/gemma-3-4b-it:free)
	51.6M tokens
	[
	Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling.
	](https://openrouter.ai/google/gemma-3-4b-it:free)
	by [google](https://openrouter.ai/google) 33K context $0/M input $0/M output
- [Gemma 3 12B (free)](https://openrouter.ai/google/gemma-3-12b-it:free)
	36.2M tokens
	[
	Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling. Gemma 3 12B is the second largest in the family of Gemma 3 models after Gemma 3 27B
	](https://openrouter.ai/google/gemma-3-12b-it:free)
	by [google](https://openrouter.ai/google) 33K context $0/M input $0/M output
- [Gemma 3n 4B (free)](https://openrouter.ai/google/gemma-3n-e4b-it:free)
	28.7M tokens
	[
	Gemma 3n E4B-it is optimized for efficient execution on mobile and low-resource devices, such as phones, laptops, and tablets. It supports multimodal inputs—including text, visual data, and audio—enabling diverse tasks such as text generation, speech recognition, translation, and image analysis. Leveraging innovations like Per-Layer Embedding (PLE) caching and the MatFormer architecture, Gemma 3n dynamically manages memory usage and computational load by selectively activating model parameters, significantly reducing runtime resource requirements. This model supports a wide linguistic range (trained in over 140 languages) and features a flexible 32K token context window. Gemma 3n can selectively load parameters, optimizing memory and computational efficiency based on the task or device capabilities, making it well-suited for privacy-focused, offline-capable applications and on-device AI solutions. Read more in the blog post
	](https://openrouter.ai/google/gemma-3n-e4b-it:free)
	by [google](https://openrouter.ai/google) 8K context $0/M input $0/M output
- [Kimi K2 0711 (free)](https://openrouter.ai/moonshotai/kimi-k2:free)
	[
	Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. Kimi K2 excels across a broad range of benchmarks, particularly in coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) tasks. It supports long-context inference up to 128K tokens and is designed with a novel training stack that includes the MuonClip optimizer for stable large-scale MoE training.
	](https://openrouter.ai/moonshotai/kimi-k2:free)
	by [moonshotai](https://openrouter.ai/moonshotai) 33K context $0/M input $0/M output
